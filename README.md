# GUI-Agent Training Framework

<div align="center">

**åŸºäº VERL å’Œ LLaMA-Factory çš„ GUI æ™ºèƒ½ä½“è®­ç»ƒæ¡†æ¶**

[English](#english) | [ä¸­æ–‡](#ä¸­æ–‡)

</div>

---

## ä¸­æ–‡

### ğŸ“– é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªç»¼åˆæ€§çš„ GUI Agent è®­ç»ƒæ¡†æ¶ï¼Œæ”¯æŒä½¿ç”¨å¤šç§è®­ç»ƒæ–¹æ³•å’Œæ¡†æ¶æ¥è®­ç»ƒè§†è§‰-è¯­è¨€æ¨¡å‹ä»¥æ‰§è¡Œ GUI æ“ä½œä»»åŠ¡ã€‚é¡¹ç›®æ•´åˆäº†ä¸šç•Œé¢†å…ˆçš„è®­ç»ƒæ¡†æ¶ï¼ŒåŒ…æ‹¬ VERLã€LLaMA-Factoryã€VLM-R1 å’Œ ms-swiftï¼Œä¸º GUI æ™ºèƒ½ä½“çš„è®­ç»ƒæä¾›äº†çµæ´»ä¸”å¼ºå¤§çš„è§£å†³æ–¹æ¡ˆã€‚

### âœ¨ ä¸»è¦ç‰¹æ€§

- ğŸ¯ **å¤šæ¡†æ¶æ”¯æŒ**ï¼šé›†æˆ VERLã€LLaMA-Factoryã€VLM-R1ã€ms-swift
- ğŸš€ **å¼ºåŒ–å­¦ä¹ è®­ç»ƒ**ï¼šæ”¯æŒ GRPO (Group Relative Policy Optimization)
- ğŸ“š **ç›‘ç£å¾®è°ƒ**ï¼šæ”¯æŒä¼ ç»Ÿ SFT (Supervised Fine-Tuning)
- ğŸ–¼ï¸ **å¤šæ¨¡æ€èƒ½åŠ›**ï¼šåŸºäº Qwen2.5-VL ç­‰è§†è§‰-è¯­è¨€æ¨¡å‹
- ğŸ”§ **çµæ´»é…ç½®**ï¼šæ”¯æŒ LoRAã€å…¨å‚æ•°å¾®è°ƒç­‰å¤šç§è®­ç»ƒæ–¹å¼
- ğŸ“Š **å®Œæ•´æ•°æ®æµ**ï¼šä»æ•°æ®é¢„å¤„ç†åˆ°æ¨¡å‹è®­ç»ƒçš„å®Œæ•´å·¥å…·é“¾

### ğŸ—ï¸ é¡¹ç›®ç»“æ„

```
Gui-Agent/
â”œâ”€â”€ Data/                          # æ•°æ®å¤„ç†å·¥å…·
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ convert_to_format.py   # æ•°æ®æ ¼å¼è½¬æ¢
â”‚   â”‚   â”œâ”€â”€ clean_data.py          # æ•°æ®æ¸…æ´—
â”‚   â”‚   â””â”€â”€ add_solution_field.py  # æ·»åŠ è§£å†³æ–¹æ¡ˆå­—æ®µ
â”‚   â””â”€â”€ hfd.sh                     # æ•°æ®ä¸‹è½½è„šæœ¬
â”œâ”€â”€ verl/                          # VERL å¼ºåŒ–å­¦ä¹ æ¡†æ¶
â”‚   â””â”€â”€ examples/
â”‚       â””â”€â”€ grpo_trainer/          # GRPO è®­ç»ƒç¤ºä¾‹
â”œâ”€â”€ LLaMA-Factory/                 # LLaMA-Factory SFT æ¡†æ¶
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ gui-r1-3k.json        # GUI è®­ç»ƒæ•°æ®
â”‚   â””â”€â”€ examples/
â”‚       â””â”€â”€ train_lora/            # LoRA è®­ç»ƒç¤ºä¾‹
â”œâ”€â”€ VLM-R1/                        # VLM-R1 å¤šæ¨¡æ€è®­ç»ƒæ¡†æ¶
â”‚   â”œâ”€â”€ run_scripts/               # è®­ç»ƒè„šæœ¬
â”‚   â””â”€â”€ src/open-r1-multimodal/    # æ ¸å¿ƒä»£ç 
â”œâ”€â”€ ms-swift/                      # MS-SWIFT è®­ç»ƒæ¡†æ¶
â””â”€â”€ 1/                             # VERL GUI è®­ç»ƒå®éªŒ
    â”œâ”€â”€ examples/
    â”‚   â”œâ”€â”€ qwen2_5_vl_7b_gui_grpo.sh
    â”‚   â””â”€â”€ baselines/
    â””â”€â”€ guir1/
        â”œâ”€â”€ inference.sh
        â””â”€â”€ eval.sh
```

### ğŸš€ å¿«é€Ÿå¼€å§‹

#### ç¯å¢ƒé…ç½®

1. **åˆ›å»º Conda ç¯å¢ƒ**

```bash
conda create -n gui-agent python=3.10
conda activate gui-agent
```

2. **å®‰è£…ä¾èµ–ï¼ˆæ ¹æ®é€‰æ‹©çš„æ¡†æ¶ï¼‰**

**ä½¿ç”¨ VERL æ¡†æ¶ï¼š**
```bash
cd verl
pip install -r requirements.txt
```

**ä½¿ç”¨ LLaMA-Factory æ¡†æ¶ï¼š**
```bash
cd LLaMA-Factory
pip install -e ".[torch,metrics]"
```

**ä½¿ç”¨ VLM-R1 æ¡†æ¶ï¼š**
```bash
cd VLM-R1
bash setup.sh
```

**ä½¿ç”¨ ms-swift æ¡†æ¶ï¼š**
```bash
cd ms-swift
pip install -r requirements.txt
```

### ğŸ“Š æ•°æ®å‡†å¤‡

#### æ•°æ®æ ¼å¼è½¬æ¢

æœ¬é¡¹ç›®æä¾›äº†å®Œæ•´çš„æ•°æ®å¤„ç†å·¥å…·é“¾ï¼Œæ”¯æŒå°† GUI-R1-3k æ•°æ®é›†è½¬æ¢ä¸ºå„æ¡†æ¶æ‰€éœ€æ ¼å¼ï¼š

```bash
cd Data/utils

# è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
python convert_to_format.py

# è½¬æ¢ä¸º Swift æ ¼å¼
python convert_to_swift_format.py

# æ•°æ®æ¸…æ´—
python clean_data.py

# æ·»åŠ è§£å†³æ–¹æ¡ˆå­—æ®µ
python add_solution_field.py
```

#### æ•°æ®æ ¼å¼è¯´æ˜

**VERL æ ¼å¼ï¼ˆParquetï¼‰ï¼š**
```python
{
    'image': str,           # å›¾åƒè·¯å¾„
    'instruction': str,     # ä»»åŠ¡æŒ‡ä»¤
    'history': str,         # å†å²æ“ä½œ
    'gt_action': str,       # æ­£ç¡®åŠ¨ä½œ
    'gt_bbox': list,        # ç›®æ ‡ä½ç½®
    'gt_input_text': str,   # è¾“å…¥æ–‡æœ¬
    'task_type': str        # ä»»åŠ¡ç±»å‹ (high/low)
}
```

**LLaMA-Factory æ ¼å¼ï¼ˆJSONï¼‰ï¼š**
```json
{
    "messages": [
        {
            "content": "<image>æ‰§è¡Œå‘½ä»¤...",
            "role": "user"
        },
        {
            "role": "assistant",
            "content": "[{'action': 'click', 'point': [x, y], 'input_text': '...'}]"
        }
    ],
    "images": ["path/to/image.png"]
}
```

### ğŸ¯ è®­ç»ƒæ–¹æ³•

#### æ–¹æ³• 1: ä½¿ç”¨ VERL è¿›è¡Œ GRPO è®­ç»ƒ

GRPO (Group Relative Policy Optimization) æ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡å¥–åŠ±æ¨¡å‹ä¼˜åŒ–ç­–ç•¥ã€‚

```bash
cd 1

# ç¼–è¾‘è„šæœ¬é…ç½®
# ä¿®æ”¹ MODEL_PATH ä¸ºä½ çš„æ¨¡å‹è·¯å¾„
# ä¿®æ”¹æ•°æ®è·¯å¾„

bash examples/qwen2_5_vl_7b_gui_grpo.sh
```

**å…³é”®å‚æ•°ï¼š**
- `model_path`: åŸºç¡€æ¨¡å‹è·¯å¾„ï¼ˆå¦‚ Qwen2.5-VL-7B-Instructï¼‰
- `data.train_files`: è®­ç»ƒæ•°æ®æ–‡ä»¶
- `worker.reward.compute_score`: å¥–åŠ±è®¡ç®—æ–¹å¼ï¼ˆr1guiï¼‰
- `data.max_pixels`: å›¾åƒæœ€å¤§åƒç´ æ•°
- `trainer.n_gpus_per_node`: æ¯èŠ‚ç‚¹ GPU æ•°é‡

#### æ–¹æ³• 2: ä½¿ç”¨ LLaMA-Factory è¿›è¡Œ SFT è®­ç»ƒ

SFT (Supervised Fine-Tuning) é€‚ç”¨äºæœ‰æ ‡æ³¨æ•°æ®çš„ç›‘ç£å­¦ä¹ åœºæ™¯ã€‚

```bash
cd LLaMA-Factory

# LoRA å¾®è°ƒ
llamafactory-cli train examples/train_lora/qwen2_5_vl_3b_gui_lora_sft.yaml

# å…¨å‚æ•°å¾®è°ƒ
llamafactory-cli train examples/train_full/qwen2_5_vl_full_sft.yaml
```

**é…ç½®è¯´æ˜ï¼ˆqwen2_5_vl_3b_gui_lora_sft.yamlï¼‰ï¼š**
```yaml
model_name_or_path: Qwen/Qwen2.5-VL-3B-Instruct
stage: sft
finetuning_type: lora
lora_rank: 8
lora_alpha: 16
dataset: gui-r1-3k
learning_rate: 1.0e-6
num_train_epochs: 1.0
per_device_train_batch_size: 1
gradient_accumulation_steps: 4
```

#### æ–¹æ³• 3: ä½¿ç”¨ VLM-R1 è¿›è¡Œ GRPO è®­ç»ƒ

VLM-R1 æ˜¯ä¸“é—¨ä¸ºè§†è§‰-è¯­è¨€æ¨¡å‹è®¾è®¡çš„ R1 é£æ ¼è®­ç»ƒæ¡†æ¶ã€‚

```bash
cd VLM-R1

# ä¿®æ”¹ run_scripts/run_grpo_gui.sh ä¸­çš„è·¯å¾„é…ç½®
# è®¾ç½® model_path, data_paths, image_folders

bash run_scripts/run_grpo_gui.sh
```

**ç‰¹ç‚¹ï¼š**
- æ”¯æŒå¤šå›¾åƒè¾“å…¥
- è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°
- æ”¯æŒ LoRA å’Œå…¨å‚æ•°è®­ç»ƒ
- æ”¯æŒå¤šèŠ‚ç‚¹è®­ç»ƒ

#### æ–¹æ³• 4: ä½¿ç”¨ ms-swift è¿›è¡Œè®­ç»ƒ

ms-swift æ˜¯ ModelScope æä¾›çš„æ¨¡å‹è®­ç»ƒæ¡†æ¶ã€‚

```bash
cd ms-swift

# æŸ¥çœ‹å¯ç”¨çš„è®­ç»ƒç¤ºä¾‹
ls examples/train_lora/
ls examples/train_full/

# è¿è¡Œè®­ç»ƒ
swift sft --model-type qwen2-vl-7b-instruct \
    --dataset gui-r1-3k \
    --output-dir output/gui-agent
```

### ğŸ“ˆ æ¨¡å‹æ¨ç†ä¸è¯„ä¼°

#### ä½¿ç”¨ VERL æ¨ç†

```bash
cd 1/guir1
bash inference.sh
```

#### ä½¿ç”¨ VERL è¯„ä¼°

```bash
cd 1/guir1
bash eval.sh
```

#### ä½¿ç”¨ LLaMA-Factory æ¨ç†

```bash
cd LLaMA-Factory

# CLI äº¤äº’å¼æ¨ç†
llamafactory-cli chat \
    --model_name_or_path path/to/checkpoint \
    --template qwen2_vl

# API éƒ¨ç½²
llamafactory-cli api \
    --model_name_or_path path/to/checkpoint \
    --template qwen2_vl
```

#### ä½¿ç”¨ VLM-R1 è¯„ä¼°

```bash
cd VLM-R1/src/eval

# è¯„ä¼° R1 æ¨¡å‹
torchrun --nproc_per_node=8 test_rec_r1.py

# è¯„ä¼°åŸºçº¿æ¨¡å‹
torchrun --nproc_per_node=8 test_rec_baseline.py
```

### ğŸ”¬ è®­ç»ƒæ¡†æ¶å¯¹æ¯”

| æ¡†æ¶ | è®­ç»ƒæ–¹æ³• | ä¼˜åŠ¿ | é€‚ç”¨åœºæ™¯ |
|------|---------|------|---------|
| **VERL** | GRPO (å¼ºåŒ–å­¦ä¹ ) | â€¢ æ”¯æŒå¥–åŠ±æ¨¡å‹ä¼˜åŒ–<br>â€¢ æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›<br>â€¢ é€‚åˆæ¢ç´¢æ€§ä»»åŠ¡ | éœ€è¦ä¼˜åŒ–ç­–ç•¥çš„å¤æ‚ GUI ä»»åŠ¡ |
| **LLaMA-Factory** | SFT (ç›‘ç£å­¦ä¹ ) | â€¢ è®­ç»ƒç¨³å®š<br>â€¢ é…ç½®ç®€å•<br>â€¢ æ”¯æŒå¤šç§å¾®è°ƒæ–¹å¼ | æœ‰æ˜ç¡®æ ‡æ³¨æ•°æ®çš„åœºæ™¯ |
| **VLM-R1** | GRPO (å¼ºåŒ–å­¦ä¹ ) | â€¢ ä¸“ä¸ºè§†è§‰-è¯­è¨€æ¨¡å‹è®¾è®¡<br>â€¢ æ”¯æŒå¤šå›¾åƒè¾“å…¥<br>â€¢ ä¸°å¯Œçš„å¥–åŠ±å‡½æ•° | å¤šæ¨¡æ€æ¨ç†å’Œè§†è§‰ç†è§£ä»»åŠ¡ |
| **ms-swift** | SFT/LoRA | â€¢ é›†æˆ ModelScope<br>â€¢ å¼€ç®±å³ç”¨<br>â€¢ ç¤¾åŒºæ”¯æŒå¥½ | å¿«é€ŸåŸå‹å¼€å‘å’Œå®éªŒ |

### ğŸ’¡ æœ€ä½³å®è·µ

#### é€‰æ‹©è®­ç»ƒæ–¹æ³•

1. **æœ‰å……è¶³æ ‡æ³¨æ•°æ®** â†’ ä½¿ç”¨ LLaMA-Factory SFT
2. **éœ€è¦ä¼˜åŒ–ç­–ç•¥** â†’ ä½¿ç”¨ VERL æˆ– VLM-R1 GRPO
3. **å¿«é€Ÿå®éªŒ** â†’ ä½¿ç”¨ ms-swift
4. **å¤šæ¨¡æ€æ¨ç†** â†’ ä½¿ç”¨ VLM-R1

#### è¶…å‚æ•°å»ºè®®

**LoRA è®­ç»ƒï¼š**
```yaml
lora_rank: 8-16
lora_alpha: 16-32 (é€šå¸¸ä¸º rank çš„ 2 å€)
learning_rate: 1e-4 ~ 5e-4
batch_size: 4-8 per device
```

**å…¨å‚æ•°è®­ç»ƒï¼š**
```yaml
learning_rate: 1e-5 ~ 1e-6
batch_size: 1-2 per device
gradient_accumulation_steps: 4-8
```

**GRPO è®­ç»ƒï¼š**
```yaml
beta: 0.01 ~ 0.04 (KL æ•£åº¦æƒé‡)
num_generations: 4-8 (æ¯æ­¥ç”Ÿæˆæ ·æœ¬æ•°)
max_completion_length: 1024-2048
```

### ğŸ› ï¸ å¸¸è§é—®é¢˜

<details>
<summary><b>Q: CUDA å†…å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ</b></summary>

**è§£å†³æ–¹æ¡ˆï¼š**
1. å‡å°‘ `per_device_train_batch_size`
2. å¢åŠ  `gradient_accumulation_steps`
3. ä½¿ç”¨ LoRA è€Œéå…¨å‚æ•°è®­ç»ƒ
4. å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼š`gradient_checkpointing: true`
5. ä½¿ç”¨ DeepSpeed ZeRO-3 é…ç½®
</details>

<details>
<summary><b>Q: å¦‚ä½•é€‰æ‹©åŸºç¡€æ¨¡å‹ï¼Ÿ</b></summary>

**æ¨èæ¨¡å‹ï¼š**
- **Qwen2.5-VL-3B-Instruct**: é€‚åˆèµ„æºå—é™åœºæ™¯ï¼Œè®­ç»ƒå¿«é€Ÿ
- **Qwen2.5-VL-7B-Instruct**: å¹³è¡¡æ€§èƒ½å’Œèµ„æºï¼Œæ¨èä½¿ç”¨
- **Qwen2.5-VL-72B-Instruct**: æœ€ä½³æ€§èƒ½ï¼Œéœ€è¦å¤§é‡ GPU
</details>

<details>
<summary><b>Q: æ•°æ®æ ¼å¼è½¬æ¢å¤±è´¥ï¼Ÿ</b></summary>

**æ£€æŸ¥æ¸…å•ï¼š**
1. ç¡®ä¿å›¾åƒè·¯å¾„æ­£ç¡®
2. æ£€æŸ¥æ•°æ®å­—æ®µå®Œæ•´æ€§
3. éªŒè¯ JSON/Parquet æ ¼å¼æ˜¯å¦æ­£ç¡®
4. æŸ¥çœ‹è½¬æ¢è„šæœ¬çš„é”™è¯¯æ—¥å¿—
</details>

<details>
<summary><b>Q: è®­ç»ƒä¸æ”¶æ•›ï¼Ÿ</b></summary>

**è°ƒè¯•æ­¥éª¤ï¼š**
1. æ£€æŸ¥å­¦ä¹ ç‡æ˜¯å¦è¿‡å¤§
2. éªŒè¯æ•°æ®è´¨é‡
3. å°è¯•æ›´å°çš„ beta å€¼ï¼ˆGRPOï¼‰
4. æ£€æŸ¥å¥–åŠ±å‡½æ•°æ˜¯å¦åˆç†
5. æŸ¥çœ‹ wandb/tensorboard è®­ç»ƒæ›²çº¿
</details>

### ğŸ“š ç›¸å…³èµ„æº

- [GUI-R1](https://github.com/ritzz-ai/GUI-R1)
- [VERL æ–‡æ¡£](./verl/README.md)
- [LLaMA-Factory æ–‡æ¡£](./LLaMA-Factory/README.md)
- [VLM-R1 æ–‡æ¡£](./VLM-R1/README.md)
- [ms-swift æ–‡æ¡£](./ms-swift/README.md)
- [Qwen3-VL å®˜æ–¹ä»“åº“](https://github.com/QwenLM/Qwen3-VL)

### ğŸ¤ è‡´è°¢

æœ¬é¡¹ç›®æ•´åˆäº†ä»¥ä¸‹å¼€æºé¡¹ç›®ï¼š

- [GUI-R1](https://github.com/ritzz-ai/GUI-R1) - GUIR1æ¨¡å‹
- [VERL](https://github.com/volcengine/verl) - å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¡†æ¶
- [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) - å¤§æ¨¡å‹å¾®è°ƒå·¥å…·
- [VLM-R1](https://github.com/om-ai-lab/VLM-R1) - è§†è§‰-è¯­è¨€æ¨¡å‹ R1 è®­ç»ƒ
- [ms-swift](https://github.com/modelscope/swift) - ModelScope è®­ç»ƒæ¡†æ¶
- [Qwen3-VL](https://github.com/QwenLM/Qwen3-VL) - è§†è§‰-è¯­è¨€åŸºç¡€æ¨¡å‹

### ğŸ“ è®¸å¯è¯

æœ¬é¡¹ç›®éµå¾ªå„å­é¡¹ç›®çš„åŸå§‹è®¸å¯è¯ã€‚

---

## English

### ğŸ“– Project Overview

This is a comprehensive GUI Agent training framework that supports multiple training methods and frameworks for training vision-language models to perform GUI operation tasks. The project integrates industry-leading training frameworks including VERL, LLaMA-Factory, VLM-R1, and ms-swift, providing a flexible and powerful solution for GUI agent training.

### âœ¨ Key Features

- ğŸ¯ **Multi-Framework Support**: Integrates VERL, LLaMA-Factory, VLM-R1, and ms-swift
- ğŸš€ **Reinforcement Learning**: Supports GRPO (Group Relative Policy Optimization)
- ğŸ“š **Supervised Fine-tuning**: Supports traditional SFT methods
- ğŸ–¼ï¸ **Multimodal Capabilities**: Based on vision-language models like Qwen2.5-VL
- ğŸ”§ **Flexible Configuration**: Supports LoRA, full fine-tuning, and more
- ğŸ“Š **Complete Pipeline**: Full toolchain from data preprocessing to model training

### ğŸ—ï¸ Project Structure

```
Gui-Agent/
â”œâ”€â”€ Data/                          # Data processing tools
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ convert_to_format.py   # Data format conversion
â”‚   â”‚   â”œâ”€â”€ clean_data.py          # Data cleaning
â”‚   â”‚   â””â”€â”€ add_solution_field.py  # Add solution field
â”‚   â””â”€â”€ hfd.sh                     # Data download script
â”œâ”€â”€ verl/                          # VERL reinforcement learning framework
â”‚   â””â”€â”€ examples/
â”‚       â””â”€â”€ grpo_trainer/          # GRPO training examples
â”œâ”€â”€ LLaMA-Factory/                 # LLaMA-Factory SFT framework
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ gui-r1-3k.json        # GUI training data
â”‚   â””â”€â”€ examples/
â”‚       â””â”€â”€ train_lora/            # LoRA training examples
â”œâ”€â”€ VLM-R1/                        # VLM-R1 multimodal training framework
â”‚   â”œâ”€â”€ run_scripts/               # Training scripts
â”‚   â””â”€â”€ src/open-r1-multimodal/    # Core code
â”œâ”€â”€ ms-swift/                      # MS-SWIFT training framework
â””â”€â”€ 1/                             # VERL GUI training experiments
    â”œâ”€â”€ examples/
    â”‚   â”œâ”€â”€ qwen2_5_vl_7b_gui_grpo.sh
    â”‚   â””â”€â”€ baselines/
    â””â”€â”€ guir1/
        â”œâ”€â”€ inference.sh
        â””â”€â”€ eval.sh
```

### ğŸš€ Quick Start

#### Environment Setup

1. **Create Conda Environment**

```bash
conda create -n gui-agent python=3.10
conda activate gui-agent
```

2. **Install Dependencies (Choose Your Framework)**

**For VERL:**
```bash
cd verl
pip install -r requirements.txt
```

**For LLaMA-Factory:**
```bash
cd LLaMA-Factory
pip install -e ".[torch,metrics]"
```

**For VLM-R1:**
```bash
cd VLM-R1
bash setup.sh
```

**For ms-swift:**
```bash
cd ms-swift
pip install -r requirements.txt
```

### ğŸ“Š Data Preparation

#### Data Format Conversion

The project provides a complete data processing toolchain to convert GUI-R1-3k dataset to required formats:

```bash
cd Data/utils

# Convert to standard format
python convert_to_format.py

# Convert to Swift format
python convert_to_swift_format.py

# Data cleaning
python clean_data.py

# Add solution field
python add_solution_field.py
```

#### Data Format Specification

**VERL Format (Parquet):**
```python
{
    'image': str,           # Image path
    'instruction': str,     # Task instruction
    'history': str,         # Action history
    'gt_action': str,       # Ground truth action
    'gt_bbox': list,        # Target location
    'gt_input_text': str,   # Input text
    'task_type': str        # Task type (high/low)
}
```

**LLaMA-Factory Format (JSON):**
```json
{
    "messages": [
        {
            "content": "<image>Execute command...",
            "role": "user"
        },
        {
            "role": "assistant",
            "content": "[{'action': 'click', 'point': [x, y], 'input_text': '...'}]"
        }
    ],
    "images": ["path/to/image.png"]
}
```

### ğŸ¯ Training Methods

#### Method 1: GRPO Training with VERL

GRPO (Group Relative Policy Optimization) is a reinforcement learning method that optimizes policy through reward models.

```bash
cd 1

# Edit script configuration
# Modify MODEL_PATH to your model path
# Modify data paths

bash examples/qwen2_5_vl_7b_gui_grpo.sh
```

**Key Parameters:**
- `model_path`: Base model path (e.g., Qwen2.5-VL-7B-Instruct)
- `data.train_files`: Training data file
- `worker.reward.compute_score`: Reward computation method (r1gui)
- `data.max_pixels`: Maximum image pixels
- `trainer.n_gpus_per_node`: Number of GPUs per node

#### Method 2: SFT Training with LLaMA-Factory

SFT (Supervised Fine-Tuning) is suitable for supervised learning scenarios with labeled data.

```bash
cd LLaMA-Factory

# LoRA fine-tuning
llamafactory-cli train examples/train_lora/qwen2_5_vl_3b_gui_lora_sft.yaml

# Full fine-tuning
llamafactory-cli train examples/train_full/qwen2_5_vl_full_sft.yaml
```

**Configuration (qwen2_5_vl_3b_gui_lora_sft.yaml):**
```yaml
model_name_or_path: Qwen/Qwen2.5-VL-3B-Instruct
stage: sft
finetuning_type: lora
lora_rank: 8
lora_alpha: 16
dataset: gui-r1-3k
learning_rate: 1.0e-6
num_train_epochs: 1.0
per_device_train_batch_size: 1
gradient_accumulation_steps: 4
```

#### Method 3: GRPO Training with VLM-R1

VLM-R1 is a R1-style training framework specifically designed for vision-language models.

```bash
cd VLM-R1

# Modify path configurations in run_scripts/run_grpo_gui.sh
# Set model_path, data_paths, image_folders

bash run_scripts/run_grpo_gui.sh
```

**Features:**
- Multi-image input support
- Customizable reward functions
- Support for LoRA and full parameter training
- Multi-node training support

#### Method 4: Training with ms-swift

ms-swift is a model training framework provided by ModelScope.

```bash
cd ms-swift

# View available training examples
ls examples/train_lora/
ls examples/train_full/

# Run training
swift sft --model-type qwen2-vl-7b-instruct \
    --dataset gui-r1-3k \
    --output-dir output/gui-agent
```

### ğŸ“ˆ Inference and Evaluation

#### Inference with VERL

```bash
cd 1/guir1
bash inference.sh
```

#### Evaluation with VERL

```bash
cd 1/guir1
bash eval.sh
```

#### Inference with LLaMA-Factory

```bash
cd LLaMA-Factory

# CLI interactive inference
llamafactory-cli chat \
    --model_name_or_path path/to/checkpoint \
    --template qwen2_vl

# API deployment
llamafactory-cli api \
    --model_name_or_path path/to/checkpoint \
    --template qwen2_vl
```

#### Evaluation with VLM-R1

```bash
cd VLM-R1/src/eval

# Evaluate R1 model
torchrun --nproc_per_node=8 test_rec_r1.py

# Evaluate baseline
torchrun --nproc_per_node=8 test_rec_baseline.py
```

### ğŸ”¬ Framework Comparison

| Framework | Method | Advantages | Use Cases |
|-----------|--------|------------|-----------|
| **VERL** | GRPO (RL) | â€¢ Reward model optimization<br>â€¢ Better generalization<br>â€¢ Good for exploration | Complex GUI tasks requiring policy optimization |
| **LLaMA-Factory** | SFT | â€¢ Stable training<br>â€¢ Simple configuration<br>â€¢ Multiple tuning methods | Scenarios with clear labeled data |
| **VLM-R1** | GRPO (RL) | â€¢ Designed for VLMs<br>â€¢ Multi-image support<br>â€¢ Rich reward functions | Multimodal reasoning and vision tasks |
| **ms-swift** | SFT/LoRA | â€¢ ModelScope integration<br>â€¢ Easy to use<br>â€¢ Good community support | Rapid prototyping and experiments |

### ğŸ’¡ Best Practices

#### Choosing Training Methods

1. **Sufficient labeled data** â†’ Use LLaMA-Factory SFT
2. **Need policy optimization** â†’ Use VERL or VLM-R1 GRPO
3. **Quick experiments** â†’ Use ms-swift
4. **Multimodal reasoning** â†’ Use VLM-R1

#### Hyperparameter Recommendations

**LoRA Training:**
```yaml
lora_rank: 8-16
lora_alpha: 16-32 (usually 2x rank)
learning_rate: 1e-4 ~ 5e-4
batch_size: 4-8 per device
```

**Full Parameter Training:**
```yaml
learning_rate: 1e-5 ~ 1e-6
batch_size: 1-2 per device
gradient_accumulation_steps: 4-8
```

**GRPO Training:**
```yaml
beta: 0.01 ~ 0.04 (KL divergence weight)
num_generations: 4-8 (samples per step)
max_completion_length: 1024-2048
```

### ğŸ› ï¸ Common Issues

<details>
<summary><b>Q: CUDA out of memory?</b></summary>

**Solutions:**
1. Reduce `per_device_train_batch_size`
2. Increase `gradient_accumulation_steps`
3. Use LoRA instead of full fine-tuning
4. Enable gradient checkpointing: `gradient_checkpointing: true`
5. Use DeepSpeed ZeRO-3 configuration
</details>

<details>
<summary><b>Q: How to choose a base model?</b></summary>

**Recommended Models:**
- **Qwen2.5-VL-3B-Instruct**: For resource-constrained scenarios, fast training
- **Qwen2.5-VL-7B-Instruct**: Balanced performance and resources (recommended)
- **Qwen2.5-VL-72B-Instruct**: Best performance, requires many GPUs
</details>

<details>
<summary><b>Q: Data format conversion failed?</b></summary>

**Checklist:**
1. Ensure image paths are correct
2. Check data field completeness
3. Verify JSON/Parquet format correctness
4. Review conversion script error logs
</details>

<details>
<summary><b>Q: Training not converging?</b></summary>

**Debugging Steps:**
1. Check if learning rate is too large
2. Verify data quality
3. Try smaller beta value (GRPO)
4. Check if reward function is reasonable
5. Review wandb/tensorboard training curves
</details>

### ğŸ“š Related Resources

- [GUI-R1](https://github.com/ritzz-ai/GUI-R1)
- [VERL Documentation](./verl/README.md)
- [LLaMA-Factory Documentation](./LLaMA-Factory/README.md)
- [VLM-R1 Documentation](./VLM-R1/README.md)
- [ms-swift Documentation](./ms-swift/README.md)
- [Qwen3-VL Official Repository](https://github.com/QwenLM/Qwen3-VL)

### ğŸ¤ Acknowledgements

This project integrates the following open-source projects:

- [GUI-R1](https://github.com/ritzz-ai/GUI-R1) - GUI-R1
- [VERL](https://github.com/volcengine/verl) - Reinforcement learning framework
- [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) - LLM fine-tuning tool
- [VLM-R1](https://github.com/om-ai-lab/VLM-R1) - Vision-language model R1 training
- [ms-swift](https://github.com/modelscope/swift) - ModelScope training framework
- [Qwen3-VL](https://github.com/QwenLM/Qwen3-VL) - Vision-language foundation model

### ğŸ“ License

This project follows the original licenses of each sub-project.

---

<div align="center">

**â­ If you find this project helpful, please consider giving it a star! â­**

</div>